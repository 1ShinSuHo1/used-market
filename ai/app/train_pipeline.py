import os
import random
import shutil
from PIL import Image, ImageOps
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import confusion_matrix, classification_report

# ============================================================
# 1ï¸âƒ£ ì„¤ì •ê°’
# ============================================================
DATA_DIR = "../data"
TRAIN_DIR = os.path.join(DATA_DIR, "train")
VAL_DIR = os.path.join(DATA_DIR, "val")
MODEL_DIR = "../models"
BATCH_SIZE = 32
EPOCHS = 10
LR = 1e-4
IMG_SIZE = 224
VAL_RATIO = 0.2
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

CLASSES = ["A", "B", "C"]

# ============================================================
# 2ï¸âƒ£ (ì „ì²˜ë¦¬) ë¹„ìœ¨ìœ ì§€+íŒ¨ë”© ë¦¬ì‚¬ì´ì¦ˆ
# ============================================================
def pad_resize_keep_ratio(img: Image.Image, size: int = 224, fill_color=(0, 0, 0)):
    img = ImageOps.exif_transpose(img)
    w, h = img.size
    scale = float(size) / max(w, h)
    new_w, new_h = int(round(w * scale)), int(round(h * scale))
    img = img.resize((new_w, new_h), Image.LANCZOS)
    canvas = Image.new("RGB", (size, size), fill_color)
    offset = ((size - new_w) // 2, (size - new_h) // 2)
    canvas.paste(img, offset)
    return canvas

def preprocess_images():
    print("ğŸ–¼ï¸ [1ë‹¨ê³„] ì´ë¯¸ì§€ í¬ê¸° í†µì¼ ì¤‘...")
    for root, _, files in os.walk(DATA_DIR):
        for fname in tqdm(files, desc=root):
            if not fname.lower().endswith((".jpg", ".jpeg", ".png")):
                continue
            fpath = os.path.join(root, fname)
            try:
                with Image.open(fpath) as im:
                    im = im.convert("RGB")
                    out = pad_resize_keep_ratio(im, IMG_SIZE)
                    out.save(fpath, quality=95)
            except Exception as e:
                print(f"âŒ ì´ë¯¸ì§€ ì˜¤ë¥˜: {fpath} ({e})")
    print("âœ… ëª¨ë“  ì´ë¯¸ì§€ê°€ 224x224(ë¹„ìœ¨ ìœ ì§€)ë¡œ í†µì¼ë¨!\n")

# ============================================================
# 3ï¸âƒ£ (ë°ì´í„° ë¶„ë¦¬) 8:2 ë¹„ìœ¨ë¡œ ì´ë™
# ============================================================
def split_dataset():
    print("ğŸ“¦ [2ë‹¨ê³„] ë°ì´í„°ì…‹ train/val ë¶„ë¦¬ ì¤‘...")

    if os.path.exists(VAL_DIR):
        shutil.rmtree(VAL_DIR)
    os.makedirs(VAL_DIR, exist_ok=True)

    random.seed(42)
    for device in os.listdir(TRAIN_DIR):
        device_path = os.path.join(TRAIN_DIR, device)
        if not os.path.isdir(device_path):
            continue

        for grade in CLASSES:
            grade_path = os.path.join(device_path, grade)
            if not os.path.isdir(grade_path):
                continue

            val_grade_path = os.path.join(VAL_DIR, device, grade)
            os.makedirs(val_grade_path, exist_ok=True)

            images = [f for f in os.listdir(grade_path) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
            random.shuffle(images)
            n_val = int(len(images) * VAL_RATIO)
            val_images = images[:n_val]

            for img in val_images:
                shutil.move(os.path.join(grade_path, img), os.path.join(val_grade_path, img))

            print(f"{device}/{grade}: ì´ {len(images)}ì¥ ì¤‘ {n_val}ì¥ì„ valë¡œ ì´ë™")
    print("âœ… train/val ë¶„ë¦¬ ì™„ë£Œ!\n")

# ============================================================
# 4ï¸âƒ£ Dataset ì •ì˜
# ============================================================
class UnifiedABCDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.samples = []
        self.transform = transform
        for device_folder in os.listdir(root_dir):
            device_path = os.path.join(root_dir, device_folder)
            if not os.path.isdir(device_path):
                continue

            for grade in CLASSES:
                grade_path = os.path.join(device_path, grade)
                if not os.path.isdir(grade_path):
                    continue

                label = CLASSES.index(grade)
                for fname in os.listdir(grade_path):
                    if fname.lower().endswith((".jpg", ".jpeg", ".png")):
                        self.samples.append((os.path.join(grade_path, fname), label))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        image = Image.open(path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

# ============================================================
# 5ï¸âƒ£ í•™ìŠµ íŒŒì´í”„ë¼ì¸ + í˜¼ë™ í–‰ë ¬
# ============================================================
def train_model():
    print("ğŸ§  [3ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

    train_tf = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(0.2, 0.2, 0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_tf = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    train_data = UnifiedABCDataset(TRAIN_DIR, transform=train_tf)
    val_data = UnifiedABCDataset(VAL_DIR, transform=val_tf)
    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)

    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(train_data)}ì¥ / ê²€ì¦ ë°ì´í„°: {len(val_data)}ì¥")

    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)
    model.classifier[1] = nn.Linear(model.last_channel, len(CLASSES))
    model = model.to(DEVICE)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    for epoch in range(EPOCHS):
        print(f"\nğŸ§© Epoch {epoch + 1}/{EPOCHS}")
        model.train()
        running_loss = 0.0

        for imgs, labels in tqdm(train_loader, desc="Training"):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        avg_train_loss = running_loss / len(train_loader)

        # ---- ê²€ì¦ ----
        model.eval()
        correct, total, val_loss = 0, 0, 0.0
        all_preds, all_labels = [], []
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total * 100
        print(f"train_loss: {avg_train_loss:.4f} | val_loss: {avg_val_loss:.4f} | val_acc: {val_acc:.2f}%")

    # ---- ì €ì¥ ----
    os.makedirs(MODEL_DIR, exist_ok=True)
    model_path = os.path.join(MODEL_DIR, "mobilenet_quality_confusion.pth")
    torch.save(model.state_dict(), model_path)
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ â†’ {model_path}")

    # ============================================================
    # 6ï¸âƒ£ í˜¼ë™ í–‰ë ¬ + ë¦¬í¬íŠ¸
    # ============================================================
    print("\nğŸ“Š [ê²°ê³¼ ë¶„ì„] í˜¼ë™ í–‰ë ¬ ë° í´ë˜ìŠ¤ë³„ ë¦¬í¬íŠ¸")
    cm = confusion_matrix(all_labels, all_preds)
    print("\nConfusion Matrix:\n", cm)
    print("\nClassification Report:\n", classification_report(all_labels, all_preds, target_names=CLASSES))

    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=CLASSES, yticklabels=CLASSES)
    plt.xlabel("ì˜ˆì¸¡ í´ë˜ìŠ¤")
    plt.ylabel("ì‹¤ì œ í´ë˜ìŠ¤")
    plt.title("í˜¼ë™ í–‰ë ¬ (Confusion Matrix)")
    plt.tight_layout()
    plt.show()

# ============================================================
# ì‹¤í–‰ ìˆœì„œ
# ============================================================
if __name__ == "__main__":
    preprocess_images()
    split_dataset()
    train_model()
